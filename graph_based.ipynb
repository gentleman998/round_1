{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5897755-5c1a-45fa-a2cf-05986262f00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "class CrimeClassifier:\n",
    "    def __init__(self, min_df=2, max_features=10000, log_dir=\"logs\"):\n",
    "        self.setup_logging(log_dir)\n",
    "        self.logger.info(\"Initializing CrimeClassifier\")\n",
    "        \n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            min_df=min_df,\n",
    "            max_features=max_features,\n",
    "            ngram_range=(1, 3),\n",
    "            analyzer='char_wb'\n",
    "        )\n",
    "        self.label_propagation_iterations = 10\n",
    "        self.similarity_threshold = 0.3\n",
    "        self.graph = None\n",
    "        self.id_to_text = {}\n",
    "        self.id_to_label = {}\n",
    "        \n",
    "        self.logger.info(f\"Initialized with min_df={min_df}, max_features={max_features}\")\n",
    "        \n",
    "    def setup_logging(self, log_dir):\n",
    "        \"\"\"Setup logging to both file and console\"\"\"\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        # Create timestamp for log file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_file = os.path.join(log_dir, f'crime_classifier_{timestamp}.log')\n",
    "        \n",
    "        # Setup logger\n",
    "        self.logger = logging.getLogger('CrimeClassifier')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # File handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(file_formatter)\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        console_formatter = logging.Formatter('%(message)s')\n",
    "        console_handler.setFormatter(console_formatter)\n",
    "        \n",
    "        # Add handlers\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "        \n",
    "        self.logger.info(f\"Logging setup complete. Log file: {log_file}\")\n",
    "        \n",
    "    def clean_text_data(self, texts):\n",
    "        \"\"\"Clean text data by removing NaN values and invalid entries\"\"\"\n",
    "        self.logger.info(f\"Starting text cleaning on {len(texts)} documents\")\n",
    "        cleaned_texts = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), total=len(texts), desc=\"Cleaning texts\"):\n",
    "            if pd.isna(text):\n",
    "                self.logger.debug(f\"Skipping NaN at index {i}\")\n",
    "                continue\n",
    "                \n",
    "            if not isinstance(text, str):\n",
    "                try:\n",
    "                    text = str(text)\n",
    "                    self.logger.debug(f\"Converted non-string to string at index {i}\")\n",
    "                except:\n",
    "                    self.logger.warning(f\"Failed to convert non-string at index {i}\")\n",
    "                    continue\n",
    "                    \n",
    "            if text.strip():\n",
    "                cleaned_texts.append(text.strip())\n",
    "                valid_indices.append(i)\n",
    "            else:\n",
    "                self.logger.debug(f\"Skipping empty string at index {i}\")\n",
    "                \n",
    "        self.logger.info(f\"Text cleaning complete. Kept {len(cleaned_texts)}/{len(texts)} documents\")\n",
    "        return cleaned_texts, valid_indices\n",
    "        \n",
    "    def create_custom_embeddings(self, texts):\n",
    "        \"\"\"Create custom embeddings using TF-IDF and enhance with domain-specific features\"\"\"\n",
    "        self.logger.info(\"Starting embedding creation\")\n",
    "        \n",
    "        # Clean the text data\n",
    "        cleaned_texts, valid_indices = self.clean_text_data(texts)\n",
    "        self.valid_indices = valid_indices\n",
    "        \n",
    "        # Base TF-IDF embeddings\n",
    "        self.logger.info(\"Creating TF-IDF embeddings\")\n",
    "        try:\n",
    "            base_embeddings = self.tfidf.fit_transform(cleaned_texts)\n",
    "            self.logger.info(f\"TF-IDF vocabulary size: {len(self.tfidf.vocabulary_)}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in TF-IDF transformation: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # Create additional features\n",
    "        self.logger.info(\"Creating enhanced features\")\n",
    "        enhanced_features = []\n",
    "        \n",
    "        for text in tqdm(cleaned_texts, desc=\"Creating enhanced features\"):\n",
    "            features = []\n",
    "            features.append(len(text))\n",
    "            features.append(len(text.split()))\n",
    "            \n",
    "            keywords = {\n",
    "                'fraud': 1.5,\n",
    "                'scam': 1.5,\n",
    "                'cyber': 1.2,\n",
    "                'online': 1.2,\n",
    "                'hack': 1.3,\n",
    "                'threat': 1.4,\n",
    "                'attack': 1.3,\n",
    "                'phish': 1.5\n",
    "            }\n",
    "            text_lower = text.lower()\n",
    "            keyword_score = sum(weight for word, weight in keywords.items() \n",
    "                              if word in text_lower)\n",
    "            features.append(keyword_score)\n",
    "            \n",
    "            enhanced_features.append(features)\n",
    "            \n",
    "        enhanced_features = normalize(np.array(enhanced_features))\n",
    "        \n",
    "        # Combine embeddings\n",
    "        self.logger.info(\"Combining embeddings\")\n",
    "        final_embeddings = np.hstack([\n",
    "            base_embeddings.toarray(),\n",
    "            enhanced_features\n",
    "        ])\n",
    "        \n",
    "        final_embeddings = normalize(final_embeddings)\n",
    "        self.logger.info(f\"Final embedding shape: {final_embeddings.shape}\")\n",
    "        \n",
    "        return final_embeddings\n",
    "    \n",
    "    def build_similarity_graph(self, embeddings, labels=None):\n",
    "        \"\"\"Build a similarity graph from embeddings\"\"\"\n",
    "        self.logger.info(\"Building similarity graph\")\n",
    "        n_samples = embeddings.shape[0]\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        self.logger.info(\"Adding nodes to graph\")\n",
    "        for i in tqdm(range(n_samples), desc=\"Adding nodes\"):\n",
    "            G.add_node(i, \n",
    "                      embedding=embeddings[i],\n",
    "                      label=labels[i] if labels is not None and i < len(labels) else None)\n",
    "        \n",
    "        # Add edges\n",
    "        self.logger.info(\"Adding edges to graph\")\n",
    "        edge_count = 0\n",
    "        with tqdm(total=(n_samples * (n_samples - 1)) // 2, desc=\"Adding edges\") as pbar:\n",
    "            for i in range(n_samples):\n",
    "                for j in range(i + 1, n_samples):\n",
    "                    similarity = 1 - cosine(embeddings[i], embeddings[j])\n",
    "                    if similarity > self.similarity_threshold:\n",
    "                        G.add_edge(i, j, weight=similarity)\n",
    "                        edge_count += 1\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        self.logger.info(f\"Graph built with {n_samples} nodes and {edge_count} edges\")\n",
    "        self.graph = G\n",
    "        return G\n",
    "    \n",
    "    def propagate_labels(self, unlabeled_indices):\n",
    "        \"\"\"Propagate labels through the graph\"\"\"\n",
    "        self.logger.info(\"Starting label propagation\")\n",
    "        if not self.graph:\n",
    "            raise ValueError(\"Graph not built yet!\")\n",
    "            \n",
    "        # Initialize distributions\n",
    "        self.logger.info(\"Initializing label distributions\")\n",
    "        label_distributions = {}\n",
    "        for node in tqdm(self.graph.nodes(), desc=\"Initializing distributions\"):\n",
    "            if node not in unlabeled_indices:\n",
    "                label = self.graph.nodes[node]['label']\n",
    "                dist = defaultdict(float)\n",
    "                dist[label] = 1.0\n",
    "                label_distributions[node] = dist\n",
    "            else:\n",
    "                label_distributions[node] = defaultdict(float)\n",
    "        \n",
    "        # Propagate labels\n",
    "        self.logger.info(f\"Propagating labels for {len(unlabeled_indices)} unlabeled nodes\")\n",
    "        for iteration in range(self.label_propagation_iterations):\n",
    "            self.logger.info(f\"Iteration {iteration + 1}/{self.label_propagation_iterations}\")\n",
    "            new_distributions = {}\n",
    "            \n",
    "            for node in tqdm(unlabeled_indices, desc=f\"Iteration {iteration + 1}\"):\n",
    "                new_dist = defaultdict(float)\n",
    "                weight_sum = 0\n",
    "                \n",
    "                for neighbor in self.graph.neighbors(node):\n",
    "                    weight = self.graph[node][neighbor]['weight']\n",
    "                    neighbor_dist = label_distributions[neighbor]\n",
    "                    \n",
    "                    for label, prob in neighbor_dist.items():\n",
    "                        new_dist[label] += weight * prob\n",
    "                    weight_sum += weight\n",
    "                \n",
    "                if weight_sum > 0:\n",
    "                    for label in new_dist:\n",
    "                        new_dist[label] /= weight_sum\n",
    "                        \n",
    "                new_distributions[node] = new_dist\n",
    "            \n",
    "            label_distributions.update(new_distributions)\n",
    "        \n",
    "        # Assign final labels\n",
    "        self.logger.info(\"Assigning final labels\")\n",
    "        final_labels = {}\n",
    "        for node in tqdm(unlabeled_indices, desc=\"Assigning labels\"):\n",
    "            dist = label_distributions[node]\n",
    "            if dist:\n",
    "                final_labels[node] = max(dist.items(), key=lambda x: x[1])[0]\n",
    "            else:\n",
    "                final_labels[node] = None\n",
    "                \n",
    "        self.logger.info(\"Label propagation complete\")\n",
    "        return final_labels\n",
    "    \n",
    "    def fit_predict(self, labeled_texts, labeled_categories, unlabeled_texts):\n",
    "        \"\"\"Main method to fit the model and predict unlabeled data\"\"\"\n",
    "        # Combine all texts\n",
    "        all_texts = labeled_texts + unlabeled_texts\n",
    "        \n",
    "        # Create embeddings and get valid indices\n",
    "        embeddings = self.create_custom_embeddings(all_texts)\n",
    "        \n",
    "        # Filter labels based on valid indices\n",
    "        n_labeled_original = len(labeled_texts)\n",
    "        valid_labeled_indices = [i for i in self.valid_indices if i < n_labeled_original]\n",
    "        filtered_categories = [labeled_categories[i] for i in valid_labeled_indices]\n",
    "        \n",
    "        # Adjust indices for unlabeled data\n",
    "        valid_unlabeled_indices = [i - n_labeled_original for i in self.valid_indices \n",
    "                                 if i >= n_labeled_original]\n",
    "        \n",
    "        # Build similarity graph\n",
    "        n_labeled = len(valid_labeled_indices)\n",
    "        all_labels = filtered_categories + [None] * (len(self.valid_indices) - n_labeled)\n",
    "        self.build_similarity_graph(embeddings, all_labels)\n",
    "        \n",
    "        # Define unlabeled indices\n",
    "        unlabeled_indices = list(range(n_labeled, len(self.valid_indices)))\n",
    "        \n",
    "        # Propagate labels\n",
    "        predictions = self.propagate_labels(unlabeled_indices)\n",
    "        \n",
    "        # Create full predictions list with None for invalid entries\n",
    "        full_predictions = [None] * len(unlabeled_texts)\n",
    "        for idx, pred_idx in enumerate(valid_unlabeled_indices):\n",
    "            if pred_idx < len(full_predictions):\n",
    "                full_predictions[pred_idx] = predictions.get(idx + n_labeled)\n",
    "        \n",
    "        return full_predictions\n",
    "    \n",
    "    def save_results(self, predictions, output_dir=\"results\"):\n",
    "        \"\"\"Save predictions and statistics to files\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Save predictions\n",
    "        predictions_file = os.path.join(output_dir, f'predictions_{timestamp}.json')\n",
    "        with open(predictions_file, 'w') as f:\n",
    "            json.dump(predictions, f, indent=2)\n",
    "        self.logger.info(f\"Predictions saved to {predictions_file}\")\n",
    "        \n",
    "        # Calculate and save statistics\n",
    "        stats = {\n",
    "            'total_predictions': len(predictions),\n",
    "            'null_predictions': predictions.count(None),\n",
    "            'unique_labels': len(set(filter(None, predictions))),\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        stats_file = os.path.join(output_dir, f'stats_{timestamp}.json')\n",
    "        with open(stats_file, 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        self.logger.info(f\"Statistics saved to {stats_file}\")\n",
    "        \n",
    "        return predictions_file, stats_file\n",
    "    \n",
    "    \n",
    "def load_labeled_data(data_dir, category_mapping_file):\n",
    "    \"\"\"\n",
    "    Load labeled data from CSV files in the specified directory\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir: str - Directory containing CSV files with labeled data\n",
    "    category_mapping_file: str - Path to JSON file containing category mapping\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (texts, categories)\n",
    "    \"\"\"\n",
    "    # Load category mapping\n",
    "    with open(category_mapping_file, 'r') as f:\n",
    "        category_mapping = json.load(f)\n",
    "    \n",
    "    # Flatten subcategories\n",
    "    subcategories = {}\n",
    "    for main_category, subs in category_mapping.items():\n",
    "        subcategories.update(subs)\n",
    "    \n",
    "    texts = []\n",
    "    categories = []\n",
    "    \n",
    "    # Process each CSV file in the directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if 'crimeinformation' in df.columns and 'subcategory' in df.columns:\n",
    "                    # Filter out NaN values\n",
    "                    df = df.dropna(subset=['crimeinformation', 'subcategory'])\n",
    "                    texts.extend(df['crimeinformation'].tolist())\n",
    "                    categories.extend(df['subcategory'].tolist())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return texts, categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b733a7a-b947-4fb5-9741-3b3e4cfc4721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging setup complete. Log file: logs/crime_classifier_20250218_130821.log\n",
      "Logging setup complete. Log file: logs/crime_classifier_20250218_130821.log\n",
      "Initializing CrimeClassifier\n",
      "Initializing CrimeClassifier\n",
      "Initialized with min_df=2, max_features=10000\n",
      "Initialized with min_df=2, max_features=10000\n",
      "Loaded 0 labeled examples\n",
      "Loaded 0 labeled examples\n",
      "Loaded 103325 unlabeled examples\n",
      "Loaded 103325 unlabeled examples\n",
      "Starting embedding creation\n",
      "Starting embedding creation\n",
      "Starting text cleaning on 103325 documents\n",
      "Starting text cleaning on 103325 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning texts: 100%|██████████| 103325/103325 [00:00<00:00, 978232.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning complete. Kept 103240/103325 documents\n",
      "Text cleaning complete. Kept 103240/103325 documents\n",
      "Creating TF-IDF embeddings\n",
      "Creating TF-IDF embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vocabulary size: 10000\n",
      "TF-IDF vocabulary size: 10000\n",
      "Creating enhanced features\n",
      "Creating enhanced features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating enhanced features: 100%|██████████| 103240/103240 [00:00<00:00, 109815.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining embeddings\n",
      "Combining embeddings\n",
      "Final embedding shape: (103240, 10003)\n",
      "Final embedding shape: (103240, 10003)\n",
      "Building similarity graph\n",
      "Building similarity graph\n",
      "Adding nodes to graph\n",
      "Adding nodes to graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding nodes: 100%|██████████| 103240/103240 [00:00<00:00, 571798.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding edges to graph\n",
      "Adding edges to graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding edges:   0%|          | 9711453/5329197180 [06:25<58:40:42, 25181.91it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m classifier\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unlabeled_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unlabeled examples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeled_categories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m pred_file, stats_file \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39msave_results(predictions)\n\u001b[1;32m     22\u001b[0m classifier\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 257\u001b[0m, in \u001b[0;36mCrimeClassifier.fit_predict\u001b[0;34m(self, labeled_texts, labeled_categories, unlabeled_texts)\u001b[0m\n\u001b[1;32m    255\u001b[0m n_labeled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_labeled_indices)\n\u001b[1;32m    256\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m filtered_categories \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_indices) \u001b[38;5;241m-\u001b[39m n_labeled)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_similarity_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Define unlabeled indices\u001b[39;00m\n\u001b[1;32m    260\u001b[0m unlabeled_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_labeled, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_indices)))\n",
      "Cell \u001b[0;32mIn[5], line 170\u001b[0m, in \u001b[0;36mCrimeClassifier.build_similarity_graph\u001b[0;34m(self, embeddings, labels)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n_samples):\n\u001b[0;32m--> 170\u001b[0m         similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mcosine\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_threshold:\n\u001b[1;32m    172\u001b[0m             G\u001b[38;5;241m.\u001b[39madd_edge(i, j, weight\u001b[38;5;241m=\u001b[39msimilarity)\n",
      "File \u001b[0;32m/opt/conda/envs/my_env/lib/python3.10/site-packages/scipy/spatial/distance.py:738\u001b[0m, in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03mCompute the Cosine distance between 1-D arrays.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    734\u001b[0m \n\u001b[1;32m    735\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# cosine distance is also referred to as 'uncentered correlation',\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m#   or 'reflective correlation'\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorrelation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/my_env/lib/python3.10/site-packages/scipy/spatial/distance.py:683\u001b[0m, in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    681\u001b[0m     vw, uw \u001b[38;5;241m=\u001b[39m v, u\n\u001b[1;32m    682\u001b[0m uv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(u, vw)\n\u001b[0;32m--> 683\u001b[0m uu \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m vv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(v, vw)\n\u001b[1;32m    685\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m uv \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(uu \u001b[38;5;241m*\u001b[39m vv)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "classifier = CrimeClassifier(log_dir=\"logs\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Load labeled data\n",
    "    data_dir = \"category_csvs/\"  \n",
    "    category_mapping_file = \"new.json\" \n",
    "\n",
    "    # Load labeled data\n",
    "    labeled_texts, labeled_categories = load_labeled_data(data_dir, category_mapping_file)\n",
    "    classifier.logger.info(f\"Loaded {len(labeled_texts)} labeled examples\")\n",
    "    unlabeled_df = pd.read_csv(\"remaining_dataset.csv\")\n",
    "    unlabeled_df = unlabeled_df.dropna(subset=['crimeaditionalinfo'])\n",
    "    unlabeled_texts = unlabeled_df['crimeaditionalinfo'].tolist()\n",
    "    classifier.logger.info(f\"Loaded {len(unlabeled_texts)} unlabeled examples\")\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = classifier.fit_predict(labeled_texts, labeled_categories, unlabeled_texts)\n",
    "\n",
    "    pred_file, stats_file = classifier.save_results(predictions)\n",
    "    classifier.logger.info(\"Processing complete!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    classifier.logger.error(f\"Error in processing: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78797467-e9d3-48f7-83f0-db086b5eafbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe26735-7d34-4dbb-ab77-4160e4cbf8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03899de3-e51d-4143-8b18-17ba8168f95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0256f-1bd7-46af-b481-637ca6a096b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-my_env-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-env-my_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

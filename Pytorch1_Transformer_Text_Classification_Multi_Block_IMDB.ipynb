{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Classifying Text with Transformers!\n",
    "### Encoder Only Architecture\n",
    "\n",
    "In this tutorial, we'll explore the versatility of attention mechanisms in handling sequential data processing tasks. Traditionally, recurrent neural networks (RNNs) and LSTM networks were commonly used for such tasks. However, with the emergence of Transformer architectures, attention mechanisms have gained prominence. We'll investigate how attention mechanisms, when used independently, efficiently capture dependencies between tokens in a sequence, eliminating the need for recurrent layers.\n",
    "\n",
    "<img src=\"../data/llm_architecture_comparison.png\" width=\"800\" align=\"center\">\n",
    "<br>\n",
    "\n",
    "[YouTube 3Blue1Brown: Visual intro to transformers](https://youtu.be/wjZofJX0v4M?si=3qnLpRegpo-0G8J-)\n",
    "<br>\n",
    "[YouTube 3Blue1Brown: Attention in transformers, visually explained](https://youtu.be/eMlx5fFNoYc?si=87IHrCt9uVDNV0fm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# We'll be using Pytorch's text library called torchtext! \n",
    "from torchtext.datasets import AG_NEWS, IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 50\n",
    "\n",
    "# Batch size for data loaders\n",
    "batch_size = 128\n",
    "\n",
    "# Maximum sequence length for text inputs\n",
    "max_len = 128\n",
    "\n",
    "# Root directory of the dataset\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4786ad",
   "metadata": {},
   "source": [
    "## Data processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using the AG News Dataset\n",
    "# Which contains a short news article and a single label to classify the \"type\" of article\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"AG_NEWS\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "# Depending on the dataset sometimes the dataset doesn't download and gives an error\n",
    "# and you'll have to download and extract manually \n",
    "# \"The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status\"\n",
    "# and there seems to be a few bugs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 1 from all labels to make them 0 and 1 (not 1 and 2...)\n",
    "# make everything lowercase\n",
    "def process_data(x):\n",
    "    return x[0] - 1, x[1].lower(), \n",
    "\n",
    "dataset_train = IMDB(root=data_set_root, split=\"train\")\n",
    "dataset_test = IMDB(root=data_set_root, split=\"test\")\n",
    "    \n",
    "dataset_train = dataset_train.map(process_data)\n",
    "dataset_test = dataset_test.map(process_data)\n",
    "\n",
    "# IMDB does not seem to be properly shuffled....\n",
    "dataset_train = dataset_train.shuffle(buffer_size=10000).set_shuffle(True)\n",
    "dataset_test = dataset_test.shuffle(buffer_size=10000).set_shuffle(True)\n",
    "\n",
    "# This is a hack to get around some random bug with the IMDB dataset not properly\n",
    "# Processing the positive (pos) datapoints, you only need to do this once...\n",
    "# This will take a few seconds..\n",
    "for label, text in dataset_train:\n",
    "    continue\n",
    "    \n",
    "for label, text in dataset_test:\n",
    "    continue\n",
    "\n",
    "# dataset_train = dataset_train.batch(batch_size)\n",
    "# dataset_test = dataset_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f586e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the training and testing datasets\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c201c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Un-Comment to train sentence-piece model for tokenizer and vocab!\n",
    "\n",
    "# from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "# with open(os.path.join(data_set_root, \"datasets/IMDB/data.txt\"), \"w\") as f2:\n",
    "#     for label, text in dataset_train:\n",
    "# #         text_only = \"\".join(line.split(\",\"))\n",
    "# #         print(text_only)\n",
    "\n",
    "# #             break\n",
    "#         f2.write(text + \"\\n\")\n",
    "            \n",
    "\n",
    "# generate_sp_model(os.path.join(data_set_root, \"datasets/IMDB/data.txt\"), \n",
    "#                   vocab_size=20000, model_prefix='spm_imdb_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eaffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the tokenizer\n",
    "# Load the SentencePiece model\n",
    "tokenizer = T.SentencePieceTokenizer(\"spm_imdb_news.model\")\n",
    "\n",
    "# Example demonstrating tokenization using the tokenizer created earlier.\n",
    "# We extract the label and text of the first item in the training dataset.\n",
    "label, text = next(iter(dataset_train))\n",
    "\n",
    "# Print the original text.\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "\n",
    "# Tokenize the text using the tokenizer.\n",
    "print(\"\\nTokenized Text:\")\n",
    "for token in tokenizer(text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c519e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build a vocabulary from the tokens yielded by the yield_tokens function\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "# <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"spm_imdb_news.vocab\"),\n",
    "                                  specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "                                  special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transform to turn text into vocab tokens\n",
    "text_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_imdb_news.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    \n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        pad_token (int): index for the <pad> token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # only replace if the token is not a special token\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        \n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c170e",
   "metadata": {},
   "source": [
    "## Create Encoder only Transformer Network\n",
    "This section of the notebook defines the components and architecture of a Transformer model designed for text classification tasks. It includes three main parts: the SinusoidalPosEmb class, which generates sinusoidal positional embeddings to encode the position of tokens in a sequence; the TransformerBlock class, which implements a single Transformer block with self-attention and feed-forward neural network layers; and the Transformer class, which assembles multiple Transformer blocks along with token embeddings and positional encodings to form a complete model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional embeds\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer normalization for the input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Multi-head self-attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, \n",
    "                                                    batch_first=True, dropout=0.25)\n",
    "        \n",
    "        # Layer normalization for the output of the attention mechanism\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Feed-forward neural network layer\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),  # Linear transformation\n",
    "            nn.LayerNorm(hidden_size),  # Layer normalization\n",
    "            nn.ELU(),  # Activation function (ELU)\n",
    "            nn.Linear(hidden_size, hidden_size)  # Linear transformation\n",
    "        )\n",
    "                \n",
    "    def forward(self, x, key_padding_mask):\n",
    "        # Layer normalization for the input\n",
    "        norm_x = self.norm1(x)\n",
    "        \n",
    "        # Multi-head self-attention mechanism\n",
    "        # [0] selects the attention output\n",
    "        attn_output = self.multihead_attn(norm_x, \n",
    "                                          norm_x, \n",
    "                                          norm_x, \n",
    "                                          key_padding_mask=key_padding_mask)[0]\n",
    "        \n",
    "        # Residual connection and layer normalization for the attention output\n",
    "        x = attn_output + x\n",
    "        norm_x = self.norm2(x)\n",
    "        \n",
    "        # Feed-forward neural network layer\n",
    "        mlp_output = self.mlp(norm_x)\n",
    "        \n",
    "        # Residual connection and output of the TransformerBlock\n",
    "        output = mlp_output + x\n",
    "        return output\n",
    "\n",
    "\n",
    "# \"Encoder-Only\" Style Transformer with self-attention\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model consisting of an embedding layer, positional embeddings, \n",
    "    multiple Transformer blocks, and a final output layer.\n",
    "    \n",
    "    Args:\n",
    "        num_emb (int): Number of embedding tokens.\n",
    "        output_size (int): Dimensionality of the output.\n",
    "        hidden_size (int): Dimensionality of the hidden layers.\n",
    "        num_layers (int): Number of Transformer blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_emb, output_size, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "#         self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.out_vec = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            input_seq (Tensor): Input sequence tensor with shape (batch_size, sequence_length).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor with shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        bs, _ = input_seq.shape\n",
    "        \n",
    "        # Create mask of padding tokens\n",
    "        key_padding_mask = input_seq == 0\n",
    "        # Add extra mask for output vec embedding\n",
    "        key_padding_mask = torch.cat((torch.zeros(bs, 1, device=input_seq.device).bool(), \n",
    "                                      key_padding_mask), 1)\n",
    "        \n",
    "        # Embed the input sequence tokens\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        \n",
    "        # Concatenate a learnable output vector to the embeddings\n",
    "        input_embs = torch.cat((self.out_vec.expand(bs, 1, -1), input_embs), 1)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add a unique embedding to each token embedding depending on its position in the sequence\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        # Pass the embeddings through each Transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, key_padding_mask)\n",
    "        \n",
    "        # Pass the first ebedding in the sequence to the final linear layer to get the output\n",
    "        return self.fc_out(embs[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec7d84",
   "metadata": {},
   "source": [
    "## Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding size\n",
    "hidden_size = 512\n",
    "\n",
    "# Create model\n",
    "tf_classifier = Transformer(num_emb=len(vocab), output_size=2, hidden_size=hidden_size, \n",
    "                            num_layers=6, num_heads=8).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# We'll decay the learning rate with a Cosine scheduler\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                    T_max=nepochs,\n",
    "                                                    eta_min=0)\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.5)\n",
    "\n",
    "# Loggers for training and testing\n",
    "training_loss_logger = []\n",
    "test_loss_logger = []\n",
    "\n",
    "training_acc_logger = []\n",
    "test_acc_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbc8ed",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize progress bar for tracking epochs\n",
    "pbar = trange(0, nepochs, leave=False, desc=\"Epoch\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "# Loop over each epoch\n",
    "for epoch in pbar:\n",
    "    train_acc_count = 0\n",
    "    test_acc_count = 0\n",
    "    \n",
    "    # Update the progress bar with current training and testing accuracy\n",
    "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Test %.2f%%' % (train_acc * 100, test_acc * 100))\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    tf_classifier.train()\n",
    "    steps = 0\n",
    "    \n",
    "    # Loop over each batch in the training dataset\n",
    "    for label, text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        bs = label.shape[0]\n",
    "        \n",
    "        # Transform the text to tokens and move to the GPU\n",
    "        text_tokens = text_tranform(list(text)).to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Randomly drop tokens to aid in regularization\n",
    "        text_tokens = td(text_tokens)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Get the model predictions\n",
    "            pred = tf_classifier(text_tokens)\n",
    "\n",
    "            # Compute the loss using cross-entropy loss\n",
    "            loss = loss_fn(pred, label)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_logger.append(loss.item())\n",
    "        \n",
    "        # Update training accuracy\n",
    "        train_acc_count += (pred.argmax(1) == label).sum()\n",
    "        steps += bs\n",
    "    \n",
    "    # Calculate average training accuracy\n",
    "    train_acc = (train_acc_count / steps).item()\n",
    "    training_acc_logger.append(train_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    tf_classifier.eval()\n",
    "    steps = 0\n",
    "    \n",
    "    # Loop over each batch in the testing dataset\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(data_loader_test, desc=\"Testing\", leave=False):\n",
    "            bs = label.shape[0]\n",
    "            \n",
    "            # Transform the text to tokens and move to the GPU\n",
    "            text_tokens = text_tranform(list(text)).to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Get the model predictions\n",
    "                pred = tf_classifier(text_tokens)\n",
    "\n",
    "                # Compute the loss using cross-entropy loss\n",
    "                loss = loss_fn(pred, label)\n",
    "                \n",
    "            test_loss_logger.append(loss.item())\n",
    "\n",
    "            # Update testing accuracy\n",
    "            test_acc_count += (pred.argmax(1) == label).sum()\n",
    "            steps += bs\n",
    "\n",
    "        # Calculate average testing accuracy\n",
    "        test_acc = (test_acc_count / steps).item()\n",
    "        test_acc_logger.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89e978",
   "metadata": {},
   "source": [
    "## Plot Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b79069",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_loss_logger)), training_loss_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_loss_logger)), test_loss_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_acc_logger)), training_acc_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_acc_logger)), test_acc_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Accuracy\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Accuracy\")\n",
    "print(\"Max Test Accuracy %.2f%%\" % (np.max(test_acc_logger) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef813fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
